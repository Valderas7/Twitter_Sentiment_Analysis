{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Servir modelo con `TensorFlow Serving`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Para servir el modelo con `TensorFlow Serving` hay que iniciar un contenedor a partir de dicha imagen enlazando el modelo en local dentro del contenedor. Es importante que el modelo esté estructurado bajo una carpeta raíz que simboliza la versión de un modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <img src=\"https://i.ibb.co/4M9BR2P/Captura-de-pantalla-2024-05-12-040608.png\" alt=\"Estructura modelo\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se trata de la versión `1` del modelo. Por eso existe una carpeta con el mismo nombre. Si no fuera así y no existiera dicha carpeta, el modelo no se serviría correctamente dentro del contenedor de `Docker`. Una vez se tiene esto en cuenta, se inicia dicho contenedor a partir de la imagen de `TensorFlow Serving`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "docker run -p 8501:8501 --name sentiment_classifier -v \"C:\\Users\\valde\\Desktop\\Twitter_Sentiment_Analysis\\mlflow\\mlruns\\413953086926394230\\025e1ff6974f412c898b91aeac09e127\\artifacts\\model\\data\\model:/models/sentiment_classifier\" -e MODEL_NAME=sentiment_classifier tensorflow/serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se inicia un contenedor con nombre `sentiment_classifier` a partir de la imagen de `tensorflow/serving` enlazando el puerto `8501` del `localhost` del PC con el puerto `8501` del contenedor, ya que `TensorFlow Serving` expone por defecto el `endpoint` tipo `REST` en el puerto `8501`, por lo que se puede consultar dicho `endpoint` en el puerto `8501` de nuestro `localhost`. Como también se puede apreciar, se monta un volumen enlazando el directorio donde está el modelo almacenado en local con el directorio `/models/sentiment_classifier` dentro del contenedor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se puede mandar una petición `POST` al puerto del `localhost` enlazado con el contenedor para obtener predicciones:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curl -d '{\"instances\": [[\"this is sad\"], [\"my dog is the best\"]]}' -X POST http://localhost:8501/v1/models/sentiment_classifier:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se quisiera hacer con código de `Python` sería algo parecido a esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n    \"predictions\": [[0.00952669699], [0.88457787]\\n    ]\\n}'\n"
     ]
    }
   ],
   "source": [
    "# Librerías\n",
    "import requests\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# URL local donde se sirven las predicciones del modelo\n",
    "url = 'http://localhost:8501/v1/models/sentiment_classifier:predict'\n",
    "\n",
    "# Header para contenido de tipo JSON\n",
    "headers = {'Content-type': 'application/json'}\n",
    "\n",
    "# Datos a los que realizar la inferencia\n",
    "X_test = json.dumps({\"instances\": [['this is sad'], ['my dog is the best']]\n",
    "                    })\n",
    "\n",
    "# Solicitud 'POST' al puerto donde el modelo está siendo expuesto con el 'header' de contenido JSON\n",
    "resp = requests.post(url, headers=headers, data=X_test)\n",
    "\n",
    "# Imprime el contenido de la respuesta\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, no se trata de un método amigable para realizar las predicciones, y por tanto, exportar a producción. Por lo tanto, se va a usar otra alternativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Despliegue (producción)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend (Tensorflow Serving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear una imagen con el modelo servido en `TensorFlow Serving` hay que copiar dicho modelo fuera de un volumen, ya que si éste esta montado en un volumen, como en el caso anterior, al crear una imagen nueva, se pierde. Por tanto, se pueden realizar cualquiera de estas dos cosas:\n",
    "\n",
    "- Hacer un `commit` a un contenedor donde el modelo esté copiado fuera de un volumen.\n",
    "- Crear un `Dockerfile` en el que a partir de una imagen base de `tensorflow/serving` se copia el modelo dentro de una subcarpeta dentro de `models` y se declare como variable de entorno una con clave `MODEL_NAME` y con el valor de la subcarpeta de `models`, exponiendo el puerto donde se sirve el modelo en una `API REST`.\n",
    "\n",
    "En este caso se opta por el segundo método, por lo que una vez creado el `Dockerfile` se crea la imagen con el siguiente comando:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker build -f .\\app\\backend\\Dockerfile -t tf_serving_sentiment_classifier:1.0 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el argumento `-t` se especifica el nombre y el `tag` de la imagen en formato `nombre:tag`; y mediante el argumento `-f` el archivo `Dockerfile` a usar. Por lo que la imagen creada se llama `tf_serving_sentiment_classifier` y el `tag` es `1.0` a partir del `Dockerfile` de la carpeta `app\\backend`. El `.` del final indica el contexto del `build`, es decir, el directorio raíz a partir de donde `Docker` buscará los archivos (en este caso, la carpeta raíz del proyecto `Twitter_Sentiment_Analysis`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se crea la imagen de `Docker`, ya se puede realizar peticiones `POST` al modelo dentro de un contenedor. Para ello se inicia uno de ellos a partir de la imagen creada, con un comando como el siguiente:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker run -p 8501:8501 --name tf_serving -v f4e9>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante este comando se inicia un contenedor a partir de la imagen creada, enlazando el puerto `8501` de nuestro `localhost` con el puerto `8501` del contenedor (el puerto donde se encuentra la `API REST` dentro del contenedor para realizar peticiones `POST`). De esta forma, si se realiza peticiones a la dirección `127.0.0.1:8501` del `localhost` se obtendrán predicciones de los sentimientos de los `tweets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de construir la imagen de Docker, ésta se sube a `Dockerhub` para tenerlo almacenado en un repositorio y poder descargar la imagen posteriormente en cualquier entorno. Para ello, primero se crea un `tag` a la imagen de `Docker` creada para especificar cual es nuestro usuario `Dockerhub` y cual sería el nombre de la imagen y el `tag` en el repositorio. Por tanto, se ejecuta el siguiente comando:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker image tag tf_serving_sentiment_classifier:1.0 xxxx/tf_serving_sentiment_classifier:1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde `xxxx` es el nombre de la cuenta de `Dockerhub`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez la imagen ya ha sido `taggeada`, se sube al repositorio de `Dockerhub`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker image push valderas7/tf_serving_sentiment_classifier:1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frontend (Streamlit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la parte de `frontend` se crea una `app` con `Streamlit` que haga consultas al `backend` de `TensorFlow Serving`. Una vez es creada se crea otro `Dockerfile` para crear una imagen que sirva la aplicación web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente se deben realizar los mismos pasos que en la imagen de `backend`, es decir: `tagear` la imagen y subirla a `Dockerhub` para que la imagen esté disponible a descargar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Kubernetes` es un sistema de orquestación de contenedores usado para el despliegue de contenedores de `Docker`. Está diseñado para administrar eficientemente y coordinar `clusters` y cargas de trabajo a gran escala en un entorno de producción. Además, ayuda a gestionar servicios contenerizados mediante la automatización en el despliegue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma, se crea un despliegue con, por ejemplo, tres `pods`, cada uno de los cuales tendrá dos contenedores (uno proveniente de la imagen de la aplicación web; y otro proveniente de la imagen de la API). Aunque si es necesario, se puede escalar dicho despliegue a más o menos `pods` dependiendo del tráfico que requiera la API y la aplicación web. Además, estos `pods` se van a exponer a través de un servicio de tipo `LoadBalancer`, de forma que la API y la aplicación web se hacen accesibles a clientes externos que están fuera del `cluster` de `Kubernetes` y además añade funcionalidad de balanceo de carga para distribuir el tráfico entre los distintos `pods`, reduciendo los efectos negativos de sobrecarga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello se crea un archivo `YAML` que define tanto el servicio como el despliegue:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Se crea el servicio\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "\n",
    "# Nombre del servicio y espacio de Kubernetes donde crear el servicio:\n",
    "metadata:\n",
    "  name: app-fullstack-airbnb-service\n",
    "  namespace: airbnb\n",
    "\n",
    "spec:\n",
    "  # Pods a los que aplicar el servicio: los pods deben tener como etiqueta la clave y el valor especificados en este campo\n",
    "  selector:\n",
    "    app: app-fullstack-airbnb-pods\n",
    "\n",
    "  # Dos puertos que tendrá el servicio (3000 para la API; y 3001 para la interfaz gráfica). Evidentemente cada puerto\n",
    "  # debe apuntar a los puertos expuestos en las imágenes (8000 en la imagen de FastAPI y 8501 en la imagen de Streamlit)\n",
    "  ports:\n",
    "    - name: backend\n",
    "      protocol: \"TCP\"\n",
    "      port: 3000\n",
    "      targetPort: 8000\n",
    "      nodePort: 30200\n",
    "    - name: frontend\n",
    "      protocol: \"TCP\"\n",
    "      port: 3001\n",
    "      targetPort: 8501\n",
    "      nodePort: 30201\n",
    "\n",
    "  # Tipo del servicio: Expone el servicio al exterior del cluster balanceando el tráfico entre los pods disponibles\n",
    "  type: LoadBalancer\n",
    "\n",
    "---\n",
    "\n",
    "# Se crea el despliegue\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "\n",
    "# Nombre del despliegue y espacio de Kubernetes donde crear el servicio:\n",
    "metadata:\n",
    "  name: app-fullstack-airbnb-deployment\n",
    "  namespace: airbnb\n",
    "\n",
    "spec:\n",
    "  # Pods a los que aplicar el despliegue: los pods deben tener como etiqueta la clave y el valor especificados en este campo\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: app-fullstack-airbnb-pods\n",
    "\n",
    "  # El despliegue de Kubernetes tendrá 3 replicas o pods\n",
    "  replicas: 3\n",
    "\n",
    "  # Plantilla del despliegue en la que se define la parte de los 'pods'\n",
    "  template:\n",
    "\n",
    "    # En los metadatos se definen las etiquetas de los 'pods'. Deben tener la misma clave y valor que la parte 'selector' del\n",
    "    # despliegue y el servicio para que ambos estén enlazados con los pods y viceversa\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: app-fullstack-airbnb-pods\n",
    "\n",
    "    # Cada 'pod' tiene dos contenedores (uno con la imagen de la API de FastAPI y otro con la imagen de la interfaz\n",
    "    # gráfica de Streamlit) con los puertos expuestos en cada una de las imágenes (8000 para la API y 8501 para la UI)\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: app-backend-airbnb-container\n",
    "          image: valderas7/app-backend-airbnb:v1.1\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          ports:\n",
    "            - containerPort: 8000\n",
    "        - name: app-frontend-airbnb-container\n",
    "          image: valderas7/app-frontend-airbnb:v1.1\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          ports:\n",
    "            - containerPort: 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, los puertos de los contenedores deben ser el `8000` para la API y el `8501` para la aplicación web, que es donde están alojadas la API y la aplicación web en cada una de las dos imágenes, y por tanto, los `targetPort` del servicio también debe ser los mismos, ya que estos serán los puertos a los que va a apuntar el servicio `LoadBalancer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el cluster local de `Minikube`, una vez creado el despliegue y el servicio, para acceder a la API y a la aplicación web hay que ejecutar el siguiente comando que devuelve dos `URLs` (una para la API y otra para la aplicación web) para conectarse al servicio que expone dichas aplicaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"> minikube -n airbnb service app-fullstack-airbnb-service </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto lo que hace es crear un túnel para el servicio `app-fullstack-airbnb-service`, creando un movimiento de datos de una red a otra. Es decir, se mueve el flujo de datos de la `IP` y de cada uno de los puertos del `nodo` de `Minikube` (`nodePort`) que usa el servicio, a la `IP` local y a dos puertos aleatorios del `localhost`, de forma que se puede consultar tanto la `API` como la aplicación web en el navegador local."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
